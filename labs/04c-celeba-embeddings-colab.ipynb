{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb38314d-a1b6-4138-8c79-4a2aa41633df",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 01/29/2026</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f0f81-de7a-4c35-8b60-4847e47b7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from glob import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import gc\n",
    "import pandas as pd \n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import base64\n",
    "import plotly.graph_objects as go      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3f3da-2904-4852-b6b7-fd6de803f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0621f1f-9942-455f-87d9-c7f43baf7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are loading a transformer neural network (more on this architecture later this term)\n",
    "# there are three components that we need: the model, the image processor, and the tokenizer\n",
    "# we'll learn more about tokenization later, for now just know that this \n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\",\n",
    "                                  dtype=torch.float16,\n",
    "                                 device_map=\"auto\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\",\n",
    "                                          dtype = torch.float16,\n",
    "                                          use_fast = False,\n",
    "                                          clean_up_tokenization_spaces = True,\n",
    "                                         device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf9dc2-18ce-402f-a295-496d76e121e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and expand our data. This is a sample from CelebA. \n",
    "# https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "# What do we make of these images? How were they collected? What are the\n",
    "# considerations we might take into an analysis of this dataset?\n",
    "!wget https://github.com/jeddobson/ENGL54.41-26W/raw/refs/heads/main/data/celeba_sample.tgz\n",
    "!tar -xf celeba_sample.tgz # extract data from tarball "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f1ea8-dcd0-466a-a1f1-cffbd5b0e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at a sample image -- the first in our directory of files\n",
    "plt.imshow(Image.open(glob(\"celeba_sample/*.jpg\")[0]))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a75808-e41d-4cc0-afb7-cdf626d87a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract neural representations for these sample 250 images -- this will take some time\n",
    "embs = []\n",
    "images = glob(\"celeba_sample/*.jpg\")\n",
    "for img in images:\n",
    "    image = Image.open(img)\n",
    "    inputs = processor(images=[image], return_tensors=\"pt\")\n",
    "    inputs = inputs['pixel_values'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(inputs).to('cpu')\n",
    "    embs.append(outputs)\n",
    "    \n",
    "    # free up some memory as we go along\n",
    "    del image, inputs\n",
    "    gc.collect()\n",
    "    \n",
    "# combine all the embeddings together in a tensor matrix\n",
    "embeddings = torch.cat(embs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b03cf3-d794-4390-bfbb-2f78e02d7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num images x embedding size\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011c22-d355-4fba-aaa2-e7c059b5fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use t-SNE to reduce to two dimensions\n",
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "# store in Pandas DataFrame\n",
    "vis = pd.DataFrame({\"x\": embeddings_2d[:, 0], \"y\": embeddings_2d[:, 1], \"label\": images})\n",
    "\n",
    "# load small images for displaying on plot\n",
    "def encode_img(img_path):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        return \"data:image/jpeg;base64,\" + base64.b64encode(f.read()).decode()\n",
    "vis['encoded_img'] = vis['label'].apply(encode_img)\n",
    "fig = go.Figure()\n",
    "\n",
    "# plot images, see plotly documentation: https://plotly.com/python/images/#adding-images-to-subplots\n",
    "for _, row in vis.iterrows():\n",
    "    fig.add_layout_image(\n",
    "        dict(\n",
    "            source=row['encoded_img'],\n",
    "            x=row['x'],\n",
    "            y=row['y'],\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            sizex=3,\n",
    "            sizey=3,\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"middle\",\n",
    "            layer=\"above\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# add invisible markers for image filename\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vis['x'],\n",
    "    y=vis['y'],\n",
    "    mode='markers',\n",
    "    marker=dict(opacity=0),\n",
    "    text=vis['label'],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# layout\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE Plot of CLIP Embeddings\",\n",
    "    width=1024,\n",
    "    height=800,\n",
    "    xaxis=dict(\n",
    "        visible=True,\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        zerolinecolor='lightgray',\n",
    "        gridcolor='lightgray',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        visible=True,\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        zerolinecolor='lightgray',\n",
    "        gridcolor='lightgray',\n",
    "    ),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d737c-7311-41ed-8b8c-6e9911d9c65b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
