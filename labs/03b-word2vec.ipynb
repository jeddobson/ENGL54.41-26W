{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022a8aee-758b-40ce-952f-728ad5ddfdcd",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 01/15/2026; Updated: 01/23/2026</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e250-f383-4e9e-9593-40147f8d9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from gensim import matutils\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68272d39-195f-41f3-982b-2ca5eee2a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google News 200 Model (a much smaller model -- 50MB)\n",
    "google_model = KeyedVectors.load_word2vec_format(\"../models/google-vectors.w2v\",\n",
    "                                                 binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045004b-f424-45fa-af08-417c8e7f3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Interview\" the model\n",
    "vocab_size, dim = google_model.vectors.shape\n",
    "print(\"vocab:\", vocab_size)\n",
    "print(\"embedding size:\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e09c3e-8a03-46a6-8def-9182954ea555",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We can automatically group embeddings together into clusters using an algorithm call k-Means. This finds clusters by partitioning data into _k_ clusters. We specify the number of clusters that we want prior to running the algorith. How to pick a value for _k_? Good question. We have to make a guess or know something about the data. It's just another sign of subjectivity and choice involved in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680c60b-51a0-4c0d-9cbc-9db0f212238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# here we'll decide that we want fifty clusters of our vectors.\n",
    "# vocab_size / 50 = means potentially many tokens/words per cluster but maybe more meaningful clusters?\n",
    "\n",
    "kmeans = KMeans(n_clusters=50, random_state=42, n_init=\"auto\")\n",
    "kmeans.fit(google_model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc23f8-b100-404e-85c3-73b2dfc198d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign cluster labels to all vectors (to all vocabulary)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# these are center of the clusters--this will not correspond to a specific\n",
    "# vector but the center of vector space comprised of vectors assigned to this\n",
    "# cluster.\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be65b5-fe20-476f-9205-f49e3faadbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize these centroids and label with the cluster id\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    marker=\"o\",       # circles are nice\n",
    "    s=6,              # size of the marker\n",
    "    linewidths=3,     # increase size\n",
    "    color=\"g\",        # green\n",
    ")\n",
    "\n",
    "# add labels to the cluster points\n",
    "for i in range(centroids.shape[0]):\n",
    "     plt.annotate(i, xy = (centroids[:, 0][i], centroids[:, 1][i]), xytext = (3, 3),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "plt.title(\"k-Means Clusters of Vector Data\")\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a4f17-74d8-4493-a4e0-dda0027502e9",
   "metadata": {},
   "source": [
    "## Interrogating a Cluster\n",
    "\n",
    "We can explore centroids that are close together: these might have similar\n",
    "semantic meanings even if the vocabulary are in distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e048456-4c76-4111-b53b-961cb51672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one looks promising. The centroids, as mentioned above, contain vector values. \n",
    "# They will be the same size as our embedding space:\n",
    "centroids[27].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda957b-c356-4730-82cb-94911628c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can query the model for vectors most similar to this imaginary location in vector space:\n",
    "google_model.most_similar(positive=[centroids[27]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b113d-99f9-4b96-9c8f-2a37a3e098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select another centroid located close to that one: \n",
    "google_model.most_similar(positive=[centroids[16]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18158ee3-835b-4fab-804b-ecd8b37edd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And another:\n",
    "google_model.most_similar(positive=[centroids[46]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcb063-e443-4100-894d-8f9a69f2222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one looks like a real outlier--why?\n",
    "google_model.most_similar(positive=[centroids[26]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4851b6c-db51-4a0b-ac38-ecf2f12ab918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some other clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42258920-0d01-44fa-b8da-f743761cda8b",
   "metadata": {},
   "source": [
    "## Plotting Neighbors in Vector Space\n",
    "\n",
    "We've already seen [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) (principal components analysis) as a technique of dimensionality reduction to reduce our high dimension (vector width of 200) data to two dimensions (x,y) for plotting; now we'll use another technique to do the same. This one is thought to work especially well with this sort of data. It is called t-distributed stochastic neighbor embedding or [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) and can make useful 2D and 3D renderings of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb9b12-773d-43b0-96ec-29aea1b476a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbors(term):\n",
    "    if term in google_model:\n",
    "        vocab = [v[0] for v in google_model.most_similar(term,topn=50)]\n",
    "        embs = np.array([google_model[v] for v in vocab])\n",
    "        tsne = TSNE(n_components=2, perplexity=2, max_iter=1000, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(torch.tensor(embs))\n",
    "        xs, ys = embeddings_2d[:, 0], embeddings_2d[:, 1]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
    "        for i, w in enumerate(vocab):\n",
    "          plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "                       textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "        plt.title(f't-sne plot of neighbors of {term}')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'{term} not found in model vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10045ed-5703-41e7-ae51-e308d8cd7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_neighbors(\"blizzard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ae555-70a5-4e5e-8df7-5616298fb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to which cluster has that token been assigned?\n",
    "cluster_labels[google_model.index_to_key.index(\"blizzard\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dab2ac-19ee-41cc-b49d-b8df02e6f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's that cluster look like?\n",
    "google_model.most_similar(positive=[centroids[18]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbb80a-0b89-40e8-b239-50f27e121712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try some more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
