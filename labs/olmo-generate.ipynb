{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2169392-3185-408c-add0-7200ca85e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2be85f-f955-41e1-879e-87d3a0a7c9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244cf92b-dd52-4140-9ec2-b3d53026c239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Olmo2ForCausalLM(\n",
       "  (model): Olmo2Model(\n",
       "    (embed_tokens): Embedding(100352, 2048, padding_idx=100277)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x Olmo2DecoderLayer(\n",
       "        (self_attn): Olmo2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "          (k_norm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Olmo2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (post_attention_layernorm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Olmo2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42102dc0-bf36-4049-ae42-fd09b705ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|user|>\n",
      "Write a short poem about Eleazar Wheelock, founder of Dartmouth College.\n",
      "When he founded Dartmouth in 1779, he wasn't sure what he was doing.\n",
      "But he felt the call of the sea and left his family behind.\n",
      "He and a group of fellow adventurers traveled 400 miles through wilderness.\n",
      "After crossing the barrier of a mountain, they founded their college in Hanover, New Hampshire.\n",
      "Eleazar Wheelock was a visionary and foresaw Dartmouth as an institution that would lead.\n",
      "Eleazar Wheelock passed away in 1813, but the legacy of his vision lives on today in Dartmouth.\n",
      "\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write a short poem about Eleazar Wheelock, founder of Dartmouth College.\"\n",
    "max_new_tokens = 128\n",
    "msg = [{\"role\":\"user\",\"content\":input_text}]\n",
    "input_ids = tokenizer.apply_chat_template(msg, \n",
    "                                          return_tensors = \"pt\",\n",
    "                                          add_generation_prompt = False)\n",
    "\n",
    "output = model.generate(input_ids.to(device), \n",
    "                        do_sample=True, \n",
    "                        max_new_tokens = max_new_tokens,\n",
    "                        temperature = 1.0, \n",
    "                        top_p = 0.95)\n",
    "\n",
    "print(tokenizer.decode(output[0], \n",
    "                       skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a36c60-04af-4c49-9f9d-2f1c1e1ba7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
