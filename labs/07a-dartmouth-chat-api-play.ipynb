{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d95d96-57db-4c5f-9677-7cd781b031ef",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 02/18/2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61683abc-1dfc-48c8-a1c7-2514ea7b98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f202603-4eec-488b-aa14-778e6075d9d2",
   "metadata": {},
   "source": [
    "## OpenAI API and Dartmouth Chat\n",
    "\n",
    "In order to use this API, we'll need to obtain our API key from DartmoutChat. \n",
    "1. Visit https://chat.dartmouth.edu and login\n",
    "2. Click on the person icon (far right, top corner)\n",
    "3. Select \"Settings\"\n",
    "4. Select \"Account\"\n",
    "5. Select API keys. Click \"Show\" and cut-and-pase string below (in quotes) as the value of api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7300342-0127-4322-8ba5-9286c83492d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll test this by select the GPT OSS 120B model. This is OpenAI's \"open weights\" model. \n",
    "# it is running locally on GPU hardware at Dartmouth. Nothing leaves campus. There are no\n",
    "# token limits with using this local model.\n",
    "\n",
    "model_name = \"openai.gpt-oss-120b\"\n",
    "api_key = \"API_KEY_GOES_HERE\"\n",
    "\n",
    "client = OpenAI(base_url=\"https://chat.dartmouth.edu/api\", \n",
    "                api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003ec26-5df4-4e24-9f69-35815a0d37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make sure that we are authenticated correctly and obtain a list of models. Running\n",
    "# this cell should display a list of models. You'll see some familiar ones here along with \n",
    "# some others that are more specialized and less well-known.\n",
    "response = requests.get(\n",
    "    \"https://chat.dartmouth.edu/api/models\",\n",
    "    headers={\"Authorization\": \"bearer \" + api_key}, \n",
    ")\n",
    "models = sorted([model['id'] for model in response.json()[\"data\"] ])\n",
    "pprint(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae78d7-dcc1-48b8-bd4d-5d9fc61f0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation function -- this will submit our prompt to the API with a system prompt.\n",
    "def generate(prompt):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model = model_name,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. You are also a creative writer.\" },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        logprobs = True,\n",
    "        stream = False)\n",
    "    return chat_completion\n",
    "\n",
    "# we'll use this function later to help us interpret generated tokens.\n",
    "def colorize(logprobs):\n",
    "    tokens = [l[0] for l in logprobs]\n",
    "    color_array = np.array([l[1] for l in logprobs])\n",
    "    color_array = np.exp(color_array)\n",
    "    norm = mcolors.Normalize(vmin=color_array.min(), vmax=color_array.max())\n",
    "    scaled = 1 - norm(color_array)\n",
    "    cmap = matplotlib.colormaps['Greys']\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for tk, color in zip(tokens, scaled):\n",
    "        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n",
    "        colored_string += template.format(color, '&nbsp' + tk + '&nbsp')\n",
    "    return colored_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90dd65-cc7a-4977-bf67-e6edb3912608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output for processing its individual components separately\n",
    "output = generate(\"Write a short story about the Robert Frost statue at Dartmouth College.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4dd3c-ad41-467b-a557-49926fceea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display \"usage\" information. How many tokens were generated? How many did we submit \n",
    "# as part of prompt?\n",
    "output.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77ce55-8ba3-4890-bd83-c60fd70a9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if our model is a \"reasoning\" model, the trace will be returned separately\n",
    "reasoning_trace = output.choices[0].message.reasoning_content\n",
    "\n",
    "# message content is the output after the end of the reasoning trace.\n",
    "response = output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a698171-6f05-46ef-9b48-579b0f0a5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the reasoning trace first to see how the model has been fine-tuned to insert \n",
    "# additional \"prompts\" in the form of these generated tokens:\n",
    "print(reasoning_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3091ede-8c72-4b0c-acc6-8d65b98e702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is our response after that preamble reasoning trace:\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42cce5-6cbb-43bd-b84f-d37ff2701fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've requested that the model return \"logprobs\" for the generated output. This will give \n",
    "# us some sense of the uncertainty in the outputs. Lower values will indicate a lower \n",
    "# probability. These have been preprocessed by softmax for us through the API on the\n",
    "# server side. We'll collect the tokens and their probabilities.\n",
    "logprobs = [[ctlp.token, ctlp.logprob] for ctlp in output.choices[0].logprobs.content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d956eec-7c60-401d-91bc-5cf624bfa643",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(colorize(logprobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962ec47-6d84-4785-ad86-c39e97459de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
