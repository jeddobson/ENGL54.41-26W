{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44eb4465-f67f-4414-9a85-1c4e25590207",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 01/02/2026</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24566b0-7d9b-4651-a339-1b66a770be8e",
   "metadata": {},
   "source": [
    "## Vectorization and Document Distances in the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0516ae1-f405-4a75-8527-9e5d718a9d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e9179-8d01-4677-a0b3-f892bf1c3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a text file (HathiTrust exported text) -- this is plain text\n",
    "# and was manually exported and only available for texts not in copyright\n",
    "text = open(\"../data/uiug-30112039344814-1767014814.txt\",\"rt\").read()\n",
    "\n",
    "# display the first 130 characters of this file\n",
    "print(text[:130])\n",
    "\n",
    "# preprocessing to remove HathiTrust mark-up (header & page breaks)\n",
    "text = re.sub(r'\\A.*?(?=^##)', '', text, flags=re.S | re.M)\n",
    "text = re.sub(r'^##.*\\n?', '', text, flags=re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7e0be-336f-460a-8558-90b342567c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into lines (will work well for volumes of poetry, not so much for other stuff)\n",
    "text = text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df5d2a-66b4-4a11-afc0-6d9afbe8824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now remove super short lines\n",
    "text = [l for l in text if len(l) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae6633-164f-4b95-aa07-a321ecb72816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many lines? We'll count each line as document (a row) in our document-term matrix.\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef7ca0-7dfa-439c-9b09-7944f3250fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a sample line:\n",
    "text[433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4854d8-81f7-4ed3-8a0f-95f6ed213455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer \"vectorizes\" inputs by calculating word (token) frequencies.\n",
    "# We create an instance of the vectorizer by calling it here, without arguments. \n",
    "# There are several possible arguments (and defaults!). These have important\n",
    "# consequences for what is counted. \n",
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050bc2e-7fbe-48e8-bccb-b6fa542f5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following \"fits\" our input documents (as a list of strings) to the model. \n",
    "# We'll call the model \"dtm\" for document-term matrix. Scikit-Learn does things\n",
    "# in this way to enable multiple \"fittings\" for different purposes (in predictive\n",
    "# modeling we typically have a \"training\" and \"testing\" dataset and each need to be\n",
    "# constructed in the same manner with the same parameters.\n",
    "dtm = vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5546a3-f3fa-4f20-808c-4dc6744cf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dtm = documents x vocab/tokens\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050be13-38c0-4746-8989-8eedc663920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pick a line and view it:\n",
    "print(text[323])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b4a61-0416-42b9-802b-c0ac42604dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which columns (tokens) have non-zero values?\n",
    "dtm[323,:].todense().nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c411cef-1d86-4199-8131-e20567ae8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these numerical columns correspond to which features?\n",
    "[vec.get_feature_names_out()[v] for v in dtm[323,:].todense().nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867cd6a-5f12-4544-b9c2-65ebdd7ec262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create a cosine similarity matrix of all the rows. This has\n",
    "# pair-wise distances of all the texts. The diagonal of this matrix contains\n",
    "# values for the measurement of a text compared with itself. The upper and \n",
    "# lower triangle of the matrix are the same.\n",
    "dist = cosine_similarity(dtm)\n",
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb65e8-6d64-40ad-92aa-ba39798273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance of that text compared to the first 10 texts:\n",
    "dist[323,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05397a0c-e023-4609-ab67-5342d6332710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 25 most similar texts using our distance matrix\n",
    "[text[i] for i in np.argsort(dist[323])[::-1][:25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1e762-db9f-44ba-9bec-c8e9aedbb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can find the index of a line in our original input easy enough:\n",
    "text.index(\"That catch the wind's moan in the dead of winter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79122dd4-9994-4801-9b17-891b292abe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change variable to the line number from above\n",
    "target = 315\n",
    "print(\"Original line:\",text[target],\"\\n\")\n",
    "for d in np.argsort(dist[target])[::-1][:25][1:]:\n",
    "    print(text[d],np.round(dist[target][d],5))\n",
    "    for v in np.where((dtm[target] != 0).todense() & (dtm[d] != 0).todense())[1]:\n",
    "        print(f' {vec.get_feature_names_out()[v]}: {dtm[target,v]} (a) {dtm[d,v]} (b)',end=\" \")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
