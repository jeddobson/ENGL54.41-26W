{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Notes: This notebook creates a more advanced Perceptron-style neural network using Pytorch. We'll now generate loss information that will give us insight into the learning process. We will also use an optimizer on this loss data to adjust the weights of the network. This neural network has two outputs for binary classification of two figures from the MNIST dataset of handwritten digits. \n",
    "\n",
    "<pre>Created: 07/15/2024; Revised: 01/12/2026</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn       # this is enables us to create arbitrary neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the MNIST dataset from Yann LeCun\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "with open('../data/train-images-idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    data = data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('../data/train-labels-idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "\n",
    "# display information about the dataset\n",
    "print(f'training samples: {data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the testing data (stored separately from the training data)\n",
    "with open('../data/t10k-images-idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    test_data = test_data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('../data/t10k-labels-idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "\n",
    "# display information about the dataset\n",
    "print(f'testing samples: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an image from training data from each class -- what do these look like?\n",
    "idxs = [labels.tolist().index(i) for i in range(10)]\n",
    "fig = plt.figure(figsize=(10, 5))  # width, height in inches\n",
    "for i,idx in enumerate(idxs):\n",
    "    img = fig.add_subplot(2, 5, i + 1)\n",
    "    img.imshow(data[idx].reshape(28,28).astype('uint8'), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# These are the two image classes that we want to classify with the binary classifier\n",
    "################################################################################\n",
    "class_one = 0\n",
    "class_two = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data and labels\n",
    "# reshape our selected training data, flattening the 28x28 matrix into a single vector\n",
    "y = [0] * data[labels == class_one].shape[0] + [1] * data[labels == class_two].shape[0]\n",
    "X = np.vstack((data[labels == class_one].reshape(data[labels == class_one].shape[0], 784),data[labels == class_two].reshape(data[labels == class_two].shape[0], 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information about the dataset\n",
    "print(f'training samples: {X.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating testing data\n",
    "# reshape our selected testing data, flattening the 28x28 matrix into a single vector\n",
    "y_test = [0] * test_data[test_labels == class_one].shape[0] + [1] * test_data[test_labels == class_two].shape[0]\n",
    "X_test = np.vstack((test_data[test_labels == class_one].reshape(test_data[test_labels == class_one].shape[0], 784),test_data[test_labels == class_two].reshape(test_data[test_labels == class_two].shape[0], 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information about the dataset\n",
    "print(f'testing samples: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Important Variables **\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 30\n",
    "\n",
    "# learning rate \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Perceptron as three layers: \n",
    "# input, hidden, output\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128) # our first layer (S) will take as input pixel data (28x28 = 784) and output 128 values\n",
    "        self.layer2 = nn.Linear(128, 128)       # this is our hidden layer (A) that takes 128 input values and outputs the same\n",
    "        self.layer3 = nn.Linear(128, 2)         # this is our final layer (R) that will take 128 input values and output binary values\n",
    "\n",
    "    # define forward function with non-linear activiation. This is more complicated than the \n",
    "    # simple linear activation function that we used in the last notebook. The Rectified Linear Unit\n",
    "    # or ReLU function is applied to the weights (and later updated by the optimizer) as we push\n",
    "    # our data through the network.\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.relu(self.layer1(inputs))\n",
    "        inputs = torch.relu(self.layer2(inputs))\n",
    "        outputs = self.layer3(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model with input layer size dynamically set to length of data\n",
    "print(\"Creating neural network...\")\n",
    "input_size = X[0].shape[0]\n",
    "print(\"input layer size: {0}\".format(input_size))\n",
    "model = Perceptron(input_dim = input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to our special accelerator device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data and labels to Torch tensor datatype\n",
    "training_data = torch.FloatTensor(X)\n",
    "labels = torch.LongTensor(y)\n",
    "\n",
    "test_labels = torch.LongTensor(y_test)\n",
    "testing_data = torch.FloatTensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates loss entropy for classification tasks\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# the Adam optimizer adjusts weights using gradient optimization  \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "\n",
    "loss = []\n",
    "\n",
    "# iterate through each of the training epochs\n",
    "for e in range(epochs):\n",
    "    model.zero_grad()\n",
    "    outputs = model(training_data.to(device))\n",
    "\n",
    "    # supply labels to CrossEntropyLoss\n",
    "    loss_train = loss_fn(outputs.to('cpu'), labels)\n",
    "    loss_train.backward()\n",
    "    loss.append(loss_train.item())\n",
    "    \n",
    "    # adjust weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"Epoch: {0} Loss: {1:.4f}\".format(e,loss_train.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display training \"loss\"\n",
    "\n",
    "Iterative machine learning seeks to minimize a *loss function*. That is to say, on each \n",
    "iteration the \"loss\" should generally be lower and lower. This is a single number that \n",
    "reports the difference between actual model outputs and those from our input data. \n",
    "Ideal \"convergence\" would be the lowering of this number to 0, but in real world situations\n",
    "this is highly unlikely. We plot the *learning curve* to observe the training procedure\n",
    "and tune our variables (epochs and learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This places the model in evaluation state for testing\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to predict class from output\n",
    "def predict(input_data):\n",
    "    outputs = model(input_data.to(device))\n",
    "    pred = torch.argmax(outputs)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict entire testing dataset\n",
    "scores = list()\n",
    "for i, j in enumerate(test_labels):\n",
    "    pc = predict(testing_data[i])\n",
    "    scores.append([pc,test_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confusion matrix\n",
    "tp = len([x for x in scores if x[1] == 0 and x[0] == 0]) # true positive\n",
    "fn = len([x for x in scores if x[1] == 0 and x[0] == 1]) # false negative\n",
    "tn = len([x for x in scores if x[1] == 1 and x[0] == 1]) # true negative\n",
    "fp = len([x for x in scores if x[1] == 1 and x[0] == 0]) # false positive\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = np.round(len([x for x in scores if x[0] == x[1]]) / len(scores),3)\n",
    "\n",
    "# display confusion matrix\n",
    "print(f'total model accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(\"Class {0:2d} {1:10d} {2:5d}\".format(0,tp,fn))\n",
    "print(\"Class {0:2d} {1:10d} {2:5d}\".format(1,fp,tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's display the false positives -- these are the images that\n",
    "# belong to class 0 but were classified as class 1.\n",
    "\n",
    "idxs_fp = [i for i,x in enumerate(scores) if x[1] == 0 and x[0] == 1]\n",
    "fig = plt.figure(figsize=(10, 5))  # width, height in inches\n",
    "rows = math.ceil(len(idxs_fp) / 5)\n",
    "fig, axes = plt.subplots(rows, 5, squeeze=False)\n",
    "for ax, item in zip(axes.flat, idxs_fp):\n",
    "    ax.imshow(testing_data[item].reshape(28,28), cmap='gray')\n",
    "for ax in axes.flat[len(idxs_fp):]:\n",
    "    ax.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it!\n",
    "\n",
    "Now select different pairs of numbers (changing variables above) and re-run the training and testing data through the network. What do you see in these results?\n",
    "\n",
    "For more complexity: display the false negatives testing data rather than the false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
