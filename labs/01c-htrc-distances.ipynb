{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b328def-5b0b-4e81-9d05-f6bf75bbc091",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 01/8/2026</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3e483-6fe5-45ed-a15e-4f08caaa9cf9",
   "metadata": {},
   "source": [
    "## Creating Document-Term Matrix from HathiTrust Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e96e37-810e-4710-aafd-6ae18aa5b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5fddf-6c22-40a5-908f-559d11018d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is a list of HathiTrust ids for books. These identify\n",
    "# the HTRC extracted features dataset for each text. You can find the\n",
    "# ID by visiting https://www.hathitrust.org/ and searching for a book.\n",
    "# You will need to click on the link for a specific volume from a \n",
    "# specific library. If you want a book that is under copyright \n",
    "# protection, you can change \"Item Availbility\" from \"Full View\" to \n",
    "# \"All Items\" have you have searched for a book or author. Same process\n",
    "# applies for finding the IDs (click on \"Limited (search-only)\" to find\n",
    "# ID from the url.\n",
    "\n",
    "texts = ['inu.30000114418225',\n",
    "     'mdp.39015063955069',\n",
    "     'uva.x001172111',\n",
    "     'mdp.39015053616556',\n",
    "     'uc1.b3340190',\n",
    "     'uc1.$b803019',\n",
    "     'mdp.39015004998749',\n",
    "     'mdp.39015048713369',\n",
    "     'uc1.b4451810',\n",
    "     'mdp.39015005611895',\n",
    "     'mdp.39015047442747',\n",
    "     'mdp.39015053574953',\n",
    "     'uc1.b3340141',\n",
    "     'mdp.39015016446554',\n",
    "     'mdp.39015031222196',\n",
    "     'uc1.$b114956',\n",
    "     'pst.000005961382',\n",
    "     'hvd.hxdink',\n",
    "     'wu.89016088155',\n",
    "     'hvd.hnmhl4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d443b-9c28-4a3e-a0e7-a12e034fb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build document-term matrix by page\n",
    "fr = FeatureReader(ids = texts)\n",
    "rows = []\n",
    "for vol in fr:\n",
    "    print(vol)\n",
    "    tl = vol.tokenlist(section='body', case=False, pos=True, drop_section=True)\n",
    "    tl = tl.reset_index().rename(columns={\"token\": \"lowercase\", 0: \"count\"})\n",
    "    tl[\"volume\"] = vol.id\n",
    "    rows.append(tl[[\"volume\", \"page\", \"lowercase\", \"pos\", \"count\"]])\n",
    "\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# filter for only alphabetical tokens and longer than one character\n",
    "df = df[df[\"lowercase\"].str.isalpha() & (df[\"lowercase\"].str.len() > 1)]\n",
    "\n",
    "# filter for nouns, adjectives, and verbs\n",
    "keep_pos = {\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"VB\", \"VBD\", \"VBG\", \n",
    "            \"VBN\", \"VBP\", \"VBZ\", \"JJ\", \"JJR\", \"JJS\"}\n",
    "df = df[df[\"pos\"].isin(keep_pos)]\n",
    "\n",
    "# create page_ids\n",
    "df[\"page_id\"] = df[\"volume\"].astype(str) + \":\" + df[\"page\"].astype(str)\n",
    "\n",
    "dtm_counts = (\n",
    "    df.pivot_table(index=\"page_id\",\n",
    "                   columns=\"lowercase\",\n",
    "                   values=\"count\",\n",
    "                   aggfunc=\"sum\",\n",
    "                   fill_value=0)\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43bc9df-79d9-4e5a-82a6-4a43ab0de1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629278a5-9b2b-4b5b-8a92-6e7e702ab7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "\n",
    "cols_to_drop = [c for c in stop_words if c in dtm_counts.columns]\n",
    "dtm_counts.drop(columns = cols_to_drop,\n",
    "                inplace = True)\n",
    "\n",
    "# remove low frequency terms\n",
    "a = 3 # threshold to remove rare words\n",
    "dtm_counts = dtm_counts.loc[:, dtm_counts.sum() >= a]\n",
    "print(f'reduced term count to {dtm_counts.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deacada-1368-44ff-9dbf-f8ca015b5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful indices\n",
    "feature_names = dtm_counts.columns.to_list()\n",
    "pages = dtm_counts.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616cb81-b715-4c58-b9c3-77ccb5977464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are most similar to this page?\n",
    "sample_page = 'mdp.39015005611895:45'\n",
    "\n",
    "# get the row number\n",
    "rn = pages.index(sample_page)\n",
    "print(f'row number: {rn}')\n",
    "\n",
    "# show me the vocab. \n",
    "pv = dtm_counts.loc['mdp.39015005611895:45'].to_numpy().nonzero()[0]\n",
    "print([feature_names[f] for f in pv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e7404-4a84-466b-85a1-8b0f5ccf7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create page similarity matrix\n",
    "page_similarity = cosine_similarity(dtm_counts)\n",
    "page_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05cb34-7cc2-49c8-b200-a6996f23ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to print a table of document, distance from our target,\n",
    "# and the shared vocabulary (ignoring frequency).\n",
    "target = 1579\n",
    "\n",
    "for d in np.argsort(page_similarity[target])[::-1][:25][1:]:\n",
    "    # get intersection of vocabulary between our target page and others\n",
    "    shared_vocab = np.intersect1d(pv, dtm_counts.loc[pages[d]].to_numpy().nonzero()[0])\n",
    "    print(f'{pages[d]:25} {page_similarity[target][d]:10f} {[feature_names[f] for f in shared_vocab]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dab5ee-06b9-40d9-a459-60c8a0c9c1d1",
   "metadata": {},
   "source": [
    "## Term-Document Matrix: Representing Vocabulary from the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e3574-73be-4e07-b89a-b73296322f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term similarity matrix from document-term matrix (transpose rows & columns)\n",
    "term_similarity = cosine_similarity(dtm_counts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5588c0-69eb-49ab-aecc-eebf11743db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to two dimensions (x,y) with principle components analysis (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(term_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624e746-0041-4228-964c-8c9980d3c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_similarity(term):\n",
    "    \"\"\"Simple term similarity based on converting document-term matrix into\n",
    "    term-document matrix and calculating cosine distances of the vocabulary.\n",
    "    This is going to have relatively poor performance on a small amount of data.\n",
    "    \"\"\"\n",
    "    if term in feature_names:\n",
    "        term_idx = feature_names.index(term)\n",
    "    else:\n",
    "        return 255\n",
    "\n",
    "    # simple similar terms based on context\n",
    "    similar_terms = term_similarity[term_idx]\n",
    "    similar_vocab = [feature_names[idx] for idx in np.argsort(similar_terms)[::-1][1:25]]\n",
    "    distances = np.sort(similar_terms)[::-1][1:25]\n",
    "    return [[a,b] for a,b in zip(similar_vocab,distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc02aa6-7211-4218-a327-d863cdb8a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_term_similarity(\"ebbing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c6be5-9c98-4301-a9c5-963346692798",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_term_similarity(\"dream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c5711-6fb4-4d1d-9f32-e9835fa2adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"dream\"\n",
    "\n",
    "# reduce data to just to closest neighboring terms\n",
    "words = [r[0] for r in get_term_similarity(term)]\n",
    "plot_data = pca_data[[feature_names.index(w) for w in words]]\n",
    "xs, ys = plot_data[:, 0], plot_data[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "plt.clf()\n",
    "plt.title(\"PCA of Most Similar to: \" + term)\n",
    "plt.style.use('ggplot')\n",
    "plt.scatter(xs, ys, marker = '^')\n",
    "for i, w in enumerate(words):\n",
    "     plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f608f07-a780-4f0b-8bd9-ade00243daca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
