{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2bc8fd-8761-4b39-9d15-8b31f37761ca",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 02/03/2026; Updated 02/05/2026</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc56674-3e18-474a-a745-064d5bf8d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae1ca7-031a-4e5f-8c96-e576e57b1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix-up display for exponential notation \n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494bcde-ef80-4e10-8548-3e39f360f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281848e-c7f1-4dbb-8b07-55d5f8fdfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model names are constructed from model provider + model name.\n",
    "# We're going to load 1B model from Allen Institute. This is the base\n",
    "# foundational model from the OLMo2 series with 1 billion parameters.\n",
    "model_name = \"allenai/OLMo-2-0425-1B\"\n",
    "\n",
    "# the tokenizer is tied to the model itself\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# load the model and put on the correct device\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4748ad0-ccf6-4d50-8340-cb5fc425d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show us the basic configuration of the model--how many layers, attention heads, \n",
    "# vocabulary size, embedding width, etc:\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ebb4c-6160-4ec0-8c9c-aaab1f2e23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model into evaluation state and display architecture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61e398-2ceb-407e-83c7-95449fc9df9a",
   "metadata": {},
   "source": [
    "## Generation as Prediction of Next Tokens\n",
    "\n",
    "We'll now do some very basic next-token-prediction to generate text. These outputs will not be very interesting or creative because we are going to always take the most highly predicted next token. This is basic text completion. It is also known as __greedy__ search when used in generation. What is the next most likely word in this sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438dc823-fb8d-4c08-9de6-6e33477a9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, to return to tokenization: the model/network takes as input a tensor of tokens, not language/text. \n",
    "# we need to convert our text fragments into a tensor of token ids:\n",
    "mission = \"\"\"Dartmouth educates the most promising students and prepares them for a lifetime of learning and of responsible leadership through a faculty dedicated to teaching and the creation of knowledge.\"\"\"\n",
    "print(tokenizer(mission)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38affc11-6f0d-4fa6-b0a3-cb20679e256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Dartmouth College is located in Hanover, New\"\n",
    "inputs = tokenizer(prompt,\n",
    "                   return_tensors=\"pt\").to(model.device)\n",
    "outputs = model(input_ids = inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d01780-87bb-4862-a33e-48801338be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model outputs data for the inputs + predictions for the next token\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832db245-5da3-4013-9b47-710b4ed3f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tokens = outputs.logits[:,-1,:]\n",
    "\n",
    "# sort the predicted tokens and take the most probable next token:\n",
    "pids = torch.argsort(predicted_tokens, descending = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8fff0-03bd-4efb-be83-9a65199e45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll see the predicted tokens by their token ID:\n",
    "pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2cd06-82de-4f2d-b33f-f62d301a06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give us the token with a space:\n",
    "tokenizer.decode(pids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54845c-ee06-4644-9386-122a75f3a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of curiosity...the second most probable next token?\n",
    "tokenizer.decode(pids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b62e9-4fa4-460c-bacc-8c8e3f81bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of curiosity...the third most probable next token?\n",
    "tokenizer.decode(pids[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50ffaf-6bed-40d3-849e-43112dfc6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems likely that this model has some confidence in that answer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116a845-c458-47bd-a6d7-72883b041288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_tokens(prompt, n=10):\n",
    "    \"\"\"\n",
    "    returns: decoded string of just generated tokens.\n",
    "    This function is deterministic. It will return the same outputs for the same prompt\n",
    "    because we are always going to take the most probable next token.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt,\n",
    "                        padding=True,\n",
    "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    " \n",
    "    # this iterates n times and demonstrates autoregression.\n",
    "    # On our first iteration, we have just the original inputs as supplied as 'prompt'. \n",
    "    # On each iteration through the loop we'll append the generated token to our inputs.\n",
    "    # Note that we do not need to tokenize the output because we generating tokens; we'll\n",
    "    # decode (translate back to language) the entire tensor of tokens once the loop as completed.\n",
    "    \n",
    "    for i in range(n):\n",
    "        logits = model(input_ids).logits[:, -1, :]\n",
    "        pid = torch.argsort(logits, descending=True)[:, :1]\n",
    "        input_ids =  torch.cat((input_ids, pid),dim=1)\n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9dc573-c1f8-4271-9d3f-2846e3035cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take just the next most likely token (n = 1)\n",
    "next_tokens(\"1 + 2 = \", n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229816b-364d-4152-8791-d5d7033f9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens(\"10 + 2 = \", n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405539b1-bc49-4c5a-b4ad-30f44d3d00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens(\"Hello, I am a\", n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6ed87-dd45-472e-ac5d-9b60b4393740",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens(\"Hello, I am an\", n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e403e5-b698-400f-9d07-1f165e8c8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens(\"The quick brown fox jumps over the lazy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f3e8e-dbce-4838-9568-e3eb2cff146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try some more on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31290a4-7d22-4908-a640-20795f643bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c77564-cef3-407f-9f7e-9c64c544f38c",
   "metadata": {},
   "source": [
    "## Stochastic Generation \n",
    "\n",
    "Taking the highest probability token results in rather dull outputs. To make these more interesting and to introduce the possibility of unexpected, creative, or even readable outputs, we need to vary our selection from among these tokens. Autoregression helps increase the likelihood that these outputs will make sense by generating the next token from the entire input. If we were just generating from the previous token or even a smaller number, we would not be able to generate coherent sentences. The larger the model, the better the output. With fine-tuning the models will generate token sequences that conform to human/reader preferences and that more closely resemble everyday prose.\n",
    "\n",
    "The method used below is a highly simplified method of selecting from among the distribution of probability values. This method will introduce some randomness (by selecting from the entire vocabulary) into our selection of tokens. We'll soon see other methods to control this selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93c298-8bbd-4fb9-bb1f-5ecfd85fdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_tokens_mn(prompt, n=10):\n",
    "    inp_tok = tokenizer(prompt,\n",
    "                        padding=True,\n",
    "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    input_ids = inp_tok[\"input_ids\"]\n",
    "\n",
    "    for i in range(n):\n",
    "        logits = model(input_ids).logits[:, -1, :]\n",
    "        \n",
    "        # we'll take the softmax of predictions to normalize the values\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # and from these we'll use multinomial sampling to select a token\n",
    "        pid = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # add new token to our inputs and continue loop\n",
    "        input_ids =  torch.cat((input_ids, pid),dim=1)\n",
    "        \n",
    "    # return decoded tokens\n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05dea3-378f-411d-bb7e-37dbb1e8d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens_mn(\"The quick brown fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec274423-6d9b-44d2-a9a6-de0555c360b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens_mn(\"To bake a cake: \",n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d0006-0019-41d7-a59e-299a20f8c5d9",
   "metadata": {},
   "source": [
    "## Perplexity \n",
    "\n",
    "We can use our large language models as language models to understand how they are modeling input sequences. Perplexity is a measure of the predictive capabilities of our model. The lower the perplexity, the less \"perplexed,\" we might say, the model is by the input. Tokens with higher scores were not as likely to be predicted by the model.\n",
    "\n",
    "You might use this to probe the model's predictive power. Are sentences with lower perplexity found in training data? Not necessarily. Do sentences with higher perplexity values give us original language framents? Perhaps. Can we use these values to predict whether a sentence was likely to be generated by a language model? Potentially, but not as the only features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b5792-ed8b-403a-a56a-ffb4e0cb444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(sentence):\n",
    "    \"\"\"\n",
    "    returns a dataframe of perplexity values for input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # we extract tokens in this way in case we have subword tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # strip special character for space\n",
    "    tokens = [t.replace('Ä ',' ') for t in tokens]\n",
    "\n",
    "    # inference with labels for perplexity scores\n",
    "    outputs = model(**inputs, \n",
    "                labels=inputs['input_ids'])\n",
    "\n",
    "    # obtain logprobs\n",
    "    log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # pad perplexity values for first token--we won't have predictions for it.\n",
    "    perplexities = [0]\n",
    "\n",
    "    # extract per-token perplexity scores from log_probs\n",
    "    for i in range(1, inputs['input_ids'].size(1)):\n",
    "        target_id = inputs['input_ids'][0, i]\n",
    "        target_log_prob = log_probs[0, i -1 , target_id].item()  \n",
    "        p = torch.exp(-torch.tensor(target_log_prob)).item()\n",
    "        perplexities.append(p)\n",
    "    out = pd.DataFrame({\"tokens\":tokens,\"perplexities\":perplexities})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c0001-91f4-42c0-9128-ac6ca9b4a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_perplexity(\"The quick brown fox jumps over the lazy dog\")\n",
    "df.plot(y = \"perplexities\",\n",
    "        x = \"tokens\",\n",
    "        title = \"Perplexity\",\n",
    "        figsize=(10,3))\n",
    "plt.xticks(range(len(df)), df['tokens'], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0f1c1-69f7-4546-bc11-33494333c740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
