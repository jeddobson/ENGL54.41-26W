{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42396612-eb57-48e3-805e-ddaba61b08ed",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 01/24/2026; Updated: 01/29/2026</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8836b-d38d-4c8f-94b8-5b43c7f556d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2b20f-8199-4020-b46a-b3c02bab3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb1b83-90a7-449e-888d-63067cb2fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\",\n",
    "                                             dtype = torch.float16,\n",
    "                                             device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498416d-0d8a-462c-84a4-25fa2347f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's view the architecture of this network. It has a vocabulary equal to the size\n",
    "# of the token_embedding. The position_embedding dimensions tell us how many tokens can be \n",
    "# supplied as input to the network.\n",
    "text_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef1c61-6892-4dce-a4e3-64710a8c3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can read a CSV directly from the web:\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jeddobson/ENGL54.41-26W/refs/heads/main/data/wit_v1.train.all-1percent_sample-5k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ea43a-468b-44bc-ba32-b9a78151f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at a few sample rows from this dataset:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddf316-5e25-4a5b-a806-8524d25e9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the distribution of languages in this sample?\n",
    "df['language'].value_counts().plot(kind='bar',figsize=(20, 5),title='Languages in 5k Sample of WIT 1%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ab205-0648-4fd4-b8db-a2689ad5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just the English-language captions \n",
    "en_captions = df[(df['language'] == \"en\") & (df['caption_alt_text_description'].notna())]['caption_alt_text_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d56f6-bed1-4fab-96ee-360e8af83ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has a column to note if an image is the main article image or not\n",
    "df['is_main_image'].value_counts().plot(kind='pie', autopct='%1.1f%%', title=\"Main Article Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8d0c5-8ab2-439c-b691-bff35a7bc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use some classic NLP first:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "dtm = vec.fit_transform(en_captions)\n",
    "# summarize vocabulary counts\n",
    "vocab_sums = dtm.sum(axis=0)\n",
    "# create a dictionary of frequent terms for plotting\n",
    "freq_dict = {v:vocab_sums[0, i] for v, i in vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c44a80-c4d2-4831-adcb-05e0f9252ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe and plot 25 most frequent words as a bar chart\n",
    "v_dv = pd.DataFrame.from_dict(freq_dict, orient='index', columns=[\"frequency\"])\n",
    "v_dv.sort_values(by=\"frequency\",ascending=False).iloc[:25].plot(kind='bar',title=\"Captions: Word Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e4ccf-4905-42d6-a0ff-8a1946847842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the actual encoding used by CLIP. This will return vectors that are\n",
    "# 768 values in length. This is the neural representation of the inputs. We'll \n",
    "# be discussing Transformers in more detail next week, but for now it will be \n",
    "# helpful to know that we are taking only the first 77 tokens (these are subword\n",
    "# units) and encoding these into 768 floating point numbers. \n",
    "\n",
    "inputs = tokenizer(en_captions.tolist(), \n",
    "                   padding=True, \n",
    "                   truncation = True, \n",
    "                   max_length = 77, \n",
    "                   return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = text_encoder(**inputs.to(device))\n",
    "pooled_output = outputs.last_hidden_state.mean(dim=1).to('cpu')\n",
    "print(pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb1800-833a-4ba7-b711-692cecf4d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=2, max_iter = 1000, random_state = 42)\n",
    "embeddings_2d = tsne.fit_transform(pooled_output.to('cpu'))\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "vis = pd.DataFrame({\n",
    "    'TSNE Component 1': embeddings_2d[:, 0],\n",
    "    'TSNE Component 2': embeddings_2d[:, 1],\n",
    "    'Captions': en_captions,\n",
    "})\n",
    "fig = px.scatter(vis, x = 'TSNE Component 1', \n",
    "                 y = 'TSNE Component 2',\n",
    "                 hover_name = 'Captions',\n",
    "                 hover_data = 'Captions',\n",
    "                 title = \"t-SNE Projection of Text Embeddings from CLIP\")\n",
    "fig.update_traces(mode = \"markers\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a63a10-737b-4feb-914b-670eb12cc07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540ed62-6af3-4e16-880c-403f33284648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
