{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aac5790-eed5-43b9-9574-7be4a4eb1fd0",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 01/15/2026; Updated: 01/23/2026</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fcc80-65c4-4d28-af88-79121e05d11e",
   "metadata": {},
   "source": [
    "## Create our own embeddings using HathiTrust Data\n",
    "\n",
    "We'll use a method called doc2vec that creates embeddings from documents to produce embeddings for individual words from our HathiTrust word frequency data. This allows us to continue to use the same data model for this approach that would normally require small windows of neighboring words for context. We'll use a much larger context window in order to produce these embeddings. This technique requires much more data than it would otherwise because we need to see many samples of similar large context to learn high-quality embeddings. But it does work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e250-f383-4e9e-9593-40147f8d9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.models.keyedvectors as kv\n",
    "from gensim import matutils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from htrc_features import FeatureReader\n",
    "\n",
    "from numpy import dot\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f4cc5-f175-436c-8e5a-512e1aeea56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is a list of HathiTrust ids for books. These identify\n",
    "# the HTRC extracted features dataset for each text. You can find the\n",
    "# ID by visiting https://www.hathitrust.org/ and searching for a book.\n",
    "# You will need to click on the link for a specific volume from a \n",
    "# specific library. If you want a book that is under copyright \n",
    "# protection, you can change \"Item Availbility\" from \"Full View\" to \n",
    "# \"All Items\" have you have searched for a book or author. Same process\n",
    "# applies for finding the IDs (click on \"Limited (search-only)\" to find\n",
    "# ID from the url.\n",
    "\n",
    "texts = ['mdp.39015014296548',\n",
    " 'uc1.$b100778',\n",
    " 'uc1.$b285061',\n",
    " 'uc1.b4194243',\n",
    " 'uc1.$b106074',\n",
    " 'uc1.b3117127',\n",
    " 'uc1.$b245112',\n",
    " 'uc1.$b434924',\n",
    " 'mdp.39015002194143',\n",
    " 'uc1.b3117208',\n",
    " 'uc1.b2839083',\n",
    " 'uc1.32106005763088',\n",
    " 'uc1.$b87329',\n",
    " 'inu.30000117261671',\n",
    " 'wu.89095289229',\n",
    " 'uc1.32106001535084',\n",
    " 'mdp.39015049019139',\n",
    " 'uc1.$b103178']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045004b-f424-45fa-af08-417c8e7f3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build document-term matrix by page\n",
    "fr = FeatureReader(ids = texts)\n",
    "rows = []\n",
    "for vol in fr:\n",
    "    print(vol)\n",
    "    tl = vol.tokenlist(section='body', case=False, pos=True, drop_section=True)\n",
    "    tl = tl.reset_index().rename(columns={\"token\": \"lowercase\", 0: \"count\"})\n",
    "    tl[\"volume\"] = vol.id\n",
    "    rows.append(tl[[\"volume\", \"page\", \"lowercase\", \"pos\", \"count\"]])\n",
    "\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# filter for only alphabetical tokens and longer than one character\n",
    "df = df[df[\"lowercase\"].str.isalpha() & (df[\"lowercase\"].str.len() > 1)]\n",
    "\n",
    "# create page_ids\n",
    "df[\"page_id\"] = df[\"volume\"].astype(str) + \":\" + df[\"page\"].astype(str)\n",
    "\n",
    "dtm_counts = (\n",
    "    df.pivot_table(index=\"page_id\",\n",
    "                   columns=\"lowercase\",\n",
    "                   values=\"count\",\n",
    "                   aggfunc=\"sum\",\n",
    "                   fill_value=0)\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d253004-65aa-41f9-8eea-6f21e1c1e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process downloaded features and store as TaggedDocument with a tag for page number\n",
    "# This tage is required for Doc2Vec and would normally be based on paragraphs but we\n",
    "# can only operate on pages of data from HTRC extracted features\n",
    "\n",
    "pages = list()\n",
    "for document in dtm_counts.index.to_list():\n",
    "  pages.append(np.repeat(dtm_counts.loc[document].loc[lambda x: x > 0].index, dtm_counts.loc[document].loc[lambda x: x > 0].values).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a88ab8-ad80-4746-98b9-5f3016eb2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=tokens, tags=[f\"p{i}\"])\n",
    "          for i, tokens in enumerate(pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61539b-f1f5-4757-8a61-48697cb49764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating model\")\n",
    "wvmodel = Doc2Vec(tagged_data,\n",
    "                dm = 1,              # operate on \"paragraphs\" (pages) with distributed memory model\n",
    "                vector_size = 200,   # larger vector size might produce better results but requires more time and memory\n",
    "                min_count = 2,       # drop words with very few repetitions\n",
    "                window = 150,        # larger window size needed because of extracted features\n",
    "                epochs = 10,         # default number of epochs (like did in our Perceptron networks, we'll run all data through multiple times)\n",
    "                workers = 8)         # attempt some parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b312d32-6fd2-4870-a9eb-92861ab304df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvmodel.wv.most_similar(\"crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5734f-5fba-45c2-99c2-96d25cf102ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbors(term):\n",
    "    if term in wvmodel.wv:\n",
    "        vocab = [v[0] for v in wvmodel.wv.most_similar(term,topn=50)]\n",
    "        embs = np.array([wvmodel.wv[v] for v in vocab])\n",
    "        tsne = TSNE(n_components=2, perplexity=2, max_iter=1000, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(torch.tensor(embs))\n",
    "        xs, ys = embeddings_2d[:, 0], embeddings_2d[:, 1]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
    "        for i, w in enumerate(vocab):\n",
    "          plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "                       textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "        plt.title(f't-sne plot of neighbors of {term}')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'{term} not found in model vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc95d3a-3f03-43b0-b150-f60fdb15d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_neighbors(\"crime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa081e73-d817-4514-ad64-79045e2c5b0d",
   "metadata": {},
   "source": [
    "Now go back up and read through the displayed volume information as we are building our dataset. How might you interpret these data in terms of that dataset? What other useful queries might you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4e56b-6338-4aa5-a135-705f631ad9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
