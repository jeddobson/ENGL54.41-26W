{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 08/23/2019; Revised: 01/02/2026</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Datatypes, Intro to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Jupyter code cell. This line is a comment; \n",
    "# comments are not executed by the interpreter.\n",
    " \n",
    "# Here we are assigning a variable 'title' the value of 'Critical AI',\n",
    "# this will automatically make 'title' a String.\n",
    "title = 'Critical AI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the type assigned, we can use the type() function:\n",
    "type(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see or display the value of a String (especially a short one), we can use the print() function:\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn what else you can do with strings, you can execute help(str) in this cell.\n",
    "# We are going to move on quickly now to learn something about lists. A list is a \n",
    "# collection of items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Vectors, Matrices, and Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python supports a very large number of libraries that can be loaded\n",
    "# or imported as needed. We only import the libraries that we need in \n",
    "# order to reduce the memory requirements and (possibly) prevent \n",
    "# collisions in the namespace used by various functions.\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch (torch) is a very popular library for building neural networks and \n",
    "# for deep learning. https://pytorch.org/\n",
    "# \n",
    "# It is the industry standard for working with the kinds of AI & ML technologies\n",
    "# that we will be studying this year. \n",
    "# \n",
    "# It introduces a new datatype called a tensor. Tensors are similar to the\n",
    "# arrays and matrices used by NumPy (numpy) but are designed to run on faster\n",
    "# processing devices called GPU (graphics processing units) that we will be\n",
    "# hopefully using later this term. They also can keep a record, some history,\n",
    "# of the transformations that created them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch Tensor Types](../img/pytorch-tensor-types.png)\n",
    "\n",
    "Pytorch Tensor Types from Eli Stevens et al. *Deep Learning with Pytorch* (Manning, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors and Vectorization\n",
    "\n",
    "The sociologist Adrian Mackenzie writes in [*Machine Learners: Archaeology of a Data Practice*](https://mitpress.mit.edu/author/adrian-mackenzie-8915/) (MIT Press, 2017) of the function of vectorization as a remapping of space:\n",
    "\n",
    ">\"Machine learning locates data practice in an expanding epistemic space. The expansion\n",
    "derives, I will suggest, from a specific operational diagram that maps data into a vector\n",
    "space. It vectorizes data according to axes, coordinates, and scales. Machine learners, in\n",
    "turn, inhabit a vectorized space, and their operations vectorize data...Often data are represented as a homogenous set of numbers or a continuous flowing stream. We need, however, to archaeologically examine some of the transformations that allow different shapes and densities of data, whether in the form of numbers,\n",
    "words, or images, to become machine learnable. Data in their local complexes space\n",
    "out in many different density shapes, depending on how the changes, signals, propensities,\n",
    "and norms have been generated or configured.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vector is typically thought of as a single dimension list of values, not unlike a list.\n",
    "#\n",
    "# This variable is list of floating point numbers. What's a floating point number? It's a numerical value \n",
    "# with greater precision than a integer (i.e., the int 4 vs. the float 4.3). \n",
    "vec = [4.3, 3.0, 1.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we try perform an operation on the list (on every element of the list) we will \n",
    "# most likely not get the result we want:\n",
    "vec * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting this list to an array (and treating as a vector) enables us to apply a \n",
    "# transformation to the entire vector at once:\n",
    "vec = np.array(vec)\n",
    "vec * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do other fun stuff with this vector. For example, here are some basic \n",
    "# summary statistics:\n",
    "vec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now display all these at once:\n",
    "vec_mean = vec.mean()\n",
    "vec_min = vec.min()\n",
    "vec_max = vec.max()\n",
    "print(f'mean: {vec_mean}, min: {vec_min}, max: {vec_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to now use Pytorch tensors rather than numpy arrays. This is because\n",
    "# this datatype is especially good for the sort of work we are going to do in\n",
    "# Critical AI.\n",
    "\n",
    "vec1 = torch.tensor([4.3,3.0,1.1,0.1])\n",
    "vec2 = torch.tensor([6.3,2.8,5.1,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say that these are representations of two kinds of flowers (because they are).\n",
    "# The values, let's call them features, are measures of the length and width of two \n",
    "# types of flower appendages (sepal and petal). \n",
    "#\n",
    "# How might we answer the question of how similar are these two flowers?\n",
    "#\n",
    "# One way might be to find the difference across all four feature dimensions. We can\n",
    "# take the absolute value of that difference to get a sense of how similar these two\n",
    "# samples are to each other:\n",
    "torch.abs(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also combine these 1D vectors into a 2D (tensor) matrix:\n",
    "matrix = torch.vstack([vec1,vec2])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell us about this matrix--what is it shape? How many rows and columns do we have?\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display mean values across all four feature dimensions of the \n",
    "matrix.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display standard deviation values across all four feature dimensions of the \n",
    "matrix.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better Way: The Distance Matrix\n",
    "\n",
    "Reconceptualizing our data as features in a standardize space (via vectorization) allows us to measure distances between points, where each point is a multidimensional value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance is a measurement of a straight line between two points\n",
    "# https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "# Cosine similarity is a measurement of the angle between two vectors\n",
    "# https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will look a bit funny, but here we are measuring the distance\n",
    "# on a straight line between 4 and 10. We are seeing these distances\n",
    "# as pairs. The first row displays the distance betwen 4 and 4\n",
    "# and then 4 and 10. The second begins with the later and then the former.\n",
    "#\n",
    "# This is a know as the distance matrix. As we add values, we can compare \n",
    "# distances among all the rows and columns. The top and bottom triangle \n",
    "# separated by the  diagonal measuring the distance between each item to \n",
    "# itself, thus all zeros\n",
    "#\n",
    "euclidean_distances([[4],[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the distance matrix for our separate vectors:\n",
    "euclidean_distances([vec1,vec2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now processing the matrix composed of those two stacked vectors:\n",
    "euclidean_distances(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the differences with cosine similarity:\n",
    "cosine_similarity([vec1,vec2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll create a larger matrix now. These are the \n",
    "# first two samples from three different classes\n",
    "# of Iris flowers (setosa, versicolor, virginica)\n",
    "# from the Fisher dataset.\n",
    "iris_matrix = [[5.1,3.5,1.4,0.2],\n",
    "               [4.9,3.0,1.4,0.2],\n",
    "               [7.0,3.2,4.7,1.4],\n",
    "               [6.4,3.2,4.5,1.5],\n",
    "               [6.3,3.3,6.0,2.5],\n",
    "               [5.8,2.7,5.1,1.9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = euclidean_distances(iris_matrix)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But there are better ways to view this!\n",
    "\n",
    "# import what we need to visualize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# show it!\n",
    "plt.imshow(dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a cosine DISimilarity plot by subtracting from 1\n",
    "# (making similarity items closer to 0 rather than 1) -- this will\n",
    "# make it comparable to the euclidean data above.\n",
    "\n",
    "dist = 1 - cosine_similarity(iris_matrix)\n",
    "\n",
    "# show it!\n",
    "plt.imshow(dist)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
