{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2bc8fd-8761-4b39-9d15-8b31f37761ca",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Winter 2026</center>\n",
    "<pre>Created: 02/12/2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc56674-3e18-474a-a745-064d5bf8d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494bcde-ef80-4e10-8548-3e39f360f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281848e-c7f1-4dbb-8b07-55d5f8fdfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model names are constructed from model provider + model name.\n",
    "# We're going to load 1B model from Allen Institute. This is the Instruction\n",
    "# fine-tuned model from the OLMo2 series with 1 billion parameters.\n",
    "model_name = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "\n",
    "# the tokenizer is tied to the model itself\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# load the model and put on the correct device\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4748ad0-ccf6-4d50-8340-cb5fc425d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show us the basic configuration of the model--how many layers, attention heads, \n",
    "# vocabulary size, embedding width, etc:\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ebb4c-6160-4ec0-8c9c-aaab1f2e23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model into evaluation state and display architecture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbfc50-d0bb-48b9-864b-d0c67cb8a227",
   "metadata": {},
   "source": [
    "# Keywords for Generation\n",
    "\n",
    "The following keyword definitions will be useful in helping us understand how the sampling process used during generation works.\n",
    "\n",
    "<b>Temperature</b>: A variable or parameter used in the generation of outputs that typically ranges from 0 (cold) to 2 (hot) with a default \"randomization\" of 1.0 and is used to control the selection of tokens. Setting the temperature to zero will force the generation of the same tokens for the same inputs, making the user-supplied prompt reproducible. Increasing the temperature will increase the degree of stochasticity in generation, leading to more \"creative\" outputs for the same inputs.\n",
    "\n",
    "<b>Top-k sampling</b>: A variable or parameter used in the generation of outputs. This is an integer, a whole number. The default is typically 50. This number, represented by k, restricts the range of the probability distribution. A top-k value of 5 would restrict the generation algorithm from the top 5 most probable next predicted tokens. The higher the number, the greater the diversity of language.\n",
    "\n",
    "<b>Top-p sampling</b>: A variable or parameter used in the generation of outputs. This is a floating-point value that ranges from 0 to 1.  The default is typically 1.0. This number, represented by p, is a threshold of probability values that will restrict the stochasticity of generation by selecting only tokens above the cumulative probability of p. A top-p value of .92 will include in the probability distribution only tokens with probability values that add up to this value. The higher the number, the greater the diversity of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a25cc-315b-44c4-a444-8395c58829f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntp(prompt, n=10):\n",
    "    inp_tok = tokenizer(prompt,\n",
    "                        padding=True,\n",
    "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    input_ids = inp_tok[\"input_ids\"]\n",
    "    logits = model(**inp_tok).logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1).detach()\n",
    "    vals = [[tokenizer.decode(tk.item()),\n",
    "             probs[0][tk.item()].tolist()] for tk in torch.argsort(probs, descending=True)[:, :n][0]]\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01f6d2-93ed-4137-af88-260c8165cdd0",
   "metadata": {},
   "source": [
    "## Using top-k and top-p to restrict probability space\n",
    "\n",
    "The following cells will demonstrate how we can shape the probability space with these two parameters. We'll see how these function as cut-offs to reduce the possibility of generating tokens that exceed these thresholds. Again, the language model is deterministic in that these same probabilities are generated every single time for the same inputs. Generation stochastically selects from these probabilities and we can manage that space of probability with some key parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dacdc-5868-47b7-85a8-bcc8718fe923",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p = 0.025\n",
    "top_k = 10\n",
    "tv = ntp(\"My favorite recording artist is\", n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b536577-8fbf-4b52-9d46-a776f402971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tv, columns=['Token', 'Probability'])\n",
    "df = df.set_index('Token')\n",
    "df.plot(kind='bar',figsize=(10, 5))\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Sampling from Next Token Probabilities:\\nMy favorite recording artist is ___')\n",
    "plt.axvline(top_k,color='r', linestyle='--',label=\"top-k: \" + str(top_k))\n",
    "plt.axhline(top_p,color='g', linestyle='--',label=\"top-p: \" + str(top_p))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e1779-7656-44ed-9171-89e36086b077",
   "metadata": {},
   "source": [
    "## Temperature \n",
    "\n",
    "The temperature parameter will scale the probability values before applying other mechanisms for stochastic generation. Hugging Face, the maintainers of the transformer package that we are using with these open weights models, describes the temperature parameter in their implementation as follows:\n",
    "\n",
    "<pre>\n",
    "temperature (float, optional, defaults to 1.0) — The value used to modulate the next token probabilities.\n",
    "\n",
    "How unpredictable the next selected token will be. High values (>0.8) are good for creative tasks, low values (e.g. <0.4) for tasks that require “thinking”. Requires do_sample=True.\n",
    "</pre>\n",
    "\n",
    "We'll visualize how this method can reduce peaks and reshape our probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8d623-f321-40e3-bb1c-237958d72a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various sample temperature values\n",
    "temps = [0.025, 0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "\n",
    "def rescale(p, T):\n",
    "    w = np.power(np.clip(p, 1e-12, None), 1.0 / T)\n",
    "    return w / w.sum()\n",
    "\n",
    "temp_cols = pd.concat(\n",
    "    [df['Probability'].pipe(rescale, T).rename(f\"T={T}\") for T in temps],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = pd.concat([df, temp_cols], axis=1)\n",
    "\n",
    "df[[f\"T={T}\" for T in temps]].plot(figsize=(10,5))\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Temperature Effect on Probabilities:\\nMy favorite recording artist is ___')\n",
    "plt.legend(title='Temperature')\n",
    "plt.xticks(range(len(df)), df.index.values, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c74f8-00c4-4727-9093-26a571584ded",
   "metadata": {},
   "source": [
    "## Instruction-Tuned Model without Special Tokens\n",
    "\n",
    "Here we'll generate tokens using a similar prompt as above. We'll use basic generation with the above parameters. We will not insert the special tokens that invoke additional fine-tuned behavior. \n",
    "\n",
    "How to interpret these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b44be-819b-48ee-99bb-b96e02ee1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p = 0.025\n",
    "top_k = 10\n",
    "temperature = 1.0\n",
    "\n",
    "prompt = \"Who is my favorite recording artist?\"\n",
    "max_new_tokens = 128\n",
    "\n",
    "inp_tok = tokenizer(prompt,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "\n",
    "input_ids = inp_tok[\"input_ids\"]\n",
    "output = model.generate(input_ids.to(device), \n",
    "                        do_sample=True, \n",
    "                        max_new_tokens = max_new_tokens,\n",
    "                        temperature = temperature,\n",
    "                        top_k = top_k,\n",
    "                        top_p = top_p)\n",
    "\n",
    "print(tokenizer.decode(output[0], \n",
    "                       skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a187bbe-04f5-4f68-a8e8-0003e0a31c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now edit the above and try to adjust the parameters and prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e0577-d4fd-40f2-a744-0502ac8bf497",
   "metadata": {},
   "source": [
    "## Instruction-Tuned Model with Special Tokens\n",
    "\n",
    "Now we'll use chatml (chat mark-up) and the templates that insert special tokens into our input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ca3c5-c043-4d06-8768-1cd7f48ab38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same parameters as above:\n",
    "top_p = 0.025\n",
    "top_k = 10\n",
    "temperature = 1.0\n",
    "\n",
    "prompt = \"Who is my favorite recording artist?\"\n",
    "max_new_tokens = 128\n",
    "\n",
    "# this defines the formatted message that we'll use as our input:\n",
    "msg = [{\"role\":\"user\",\"content\":prompt}]\n",
    "\n",
    "# this applies special tokens. ''add_generate_prompt'' inserts the token for ''assistant'' voice.\n",
    "# you'll see all of this decoded below.\n",
    "input_ids = tokenizer.apply_chat_template(msg, \n",
    "                                          return_tensors = \"pt\",\n",
    "                                          add_generation_prompt = True)\n",
    "\n",
    "# generate tokens:\n",
    "output = model.generate(input_ids['input_ids'].to(device), \n",
    "                        do_sample=True, \n",
    "                        max_new_tokens = max_new_tokens,\n",
    "                        temperature = temperature,\n",
    "                        top_k = top_k,\n",
    "                        top_p = top_p)\n",
    "\n",
    "print(tokenizer.decode(output[0], \n",
    "                       skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9f8ec-95a7-4c1b-91c1-b86cebe89dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now edit the above and try to adjust the parameters and prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb861ad-093a-4227-ac8a-cfd63200384a",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting\n",
    "\n",
    "Most contemporary models have been trained on chain-of-thought data so we might not need to provide a sample to direct generation. Here we'll use the first example, which is very likely to be in the training data for this model, from Wei et al. \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" (2023) arXiv:2201.11903v6. \n",
    "\n",
    "The use of triple quotes (\"\"\") here will allow us to include new lines in our prompt and help format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f556f6-1472-49f4-98d0-e5d1e905caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same parameters as above:\n",
    "top_p = 0.025\n",
    "top_k = 10\n",
    "temperature = 1.0\n",
    "\n",
    "prompt = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
    "\n",
    "max_new_tokens = 128\n",
    "\n",
    "# this defines the formatted message that we'll use as our input:\n",
    "msg = [{\"role\":\"user\",\"content\":prompt}]\n",
    "\n",
    "# this applies special tokens. ''add_generate_prompt'' inserts the token for ''assistant'' voice.\n",
    "# you'll see all of this decoded below.\n",
    "input_ids = tokenizer.apply_chat_template(msg, \n",
    "                                          return_tensors = \"pt\",\n",
    "                                          add_generation_prompt = True)\n",
    "\n",
    "# generate tokens:\n",
    "output = model.generate(input_ids['input_ids'].to(device), \n",
    "                        do_sample=True, \n",
    "                        max_new_tokens = max_new_tokens,\n",
    "                        temperature = temperature,\n",
    "                        top_k = top_k,\n",
    "                        top_p = top_p)\n",
    "\n",
    "print(tokenizer.decode(output[0], \n",
    "                       skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf08d2e-39f0-4ebd-9786-781f849455c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now edit the above and try to adjust the parameters and prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093a15b-5ec0-43c9-a42a-271c2c45c8ed",
   "metadata": {},
   "source": [
    "## Using System Prompts\n",
    "\n",
    "Adding to our generation pipeline, instruction fine-tuned models are almost always used with a system prompt. The system prompt precedes the user prompt in the sequence of inputs. It can be used to provide some guardrails. It might also be used to break through these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9bb32f-0577-480f-a3f4-0524f73de850",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Always respond in the voice of Eleazar Wheelock, founder of Dartmouth College.\"\n",
    "prompt = \"What is the purpose of liberal education?\"\n",
    "max_new_tokens = 512\n",
    "\n",
    "# The message template will now contain an additional role, that of the \"system.\" \n",
    "# Observe how this prompt is marked and inserted into the sequence of inputs.\n",
    "msg = [{\"role\":\"system\",\"content\":system_prompt},\n",
    "       {\"role\":\"user\",\"content\":prompt}]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(msg, \n",
    "                                          return_tensors = \"pt\",\n",
    "                                          add_generation_prompt = True)\n",
    "\n",
    "output = model.generate(input_ids['input_ids'].to(device), \n",
    "                        do_sample=True, \n",
    "                        max_new_tokens = max_new_tokens)\n",
    "\n",
    "print(tokenizer.decode(output[0], \n",
    "                       skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39237a-fe72-4d95-96b4-29d550196a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now edit the above and try to adjust the parameters, system prompt, and prompt. \n",
    "# Can you get the model to work around its guardrails?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
