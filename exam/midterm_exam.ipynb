{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ca173e-9c17-4f04-9f86-cfbaa4f351ef",
   "metadata": {},
   "source": [
    "# <center>ENGL 54.41 - 26W <br>Midterm Exam: Part Two</center>\n",
    "\n",
    "\n",
    "<b>Due Date</b>: Thursday, Feburary 12 at 11:59pm. Uploaded to Canvas.\n",
    "\n",
    "<b>Instructions</b>: To complete this notebook, first open in Google Colab and save to your Drive. It is absolutely __crucial__ that you save a copy so you can edit, save your work, and return to the notebook over multiple sessions. You will most likely want to use a GPU (Runtime -> Change Runtime Type -> T4/A100/etc) When you have completed the notebook and are satisfied with your responses, download to your computer. You'll then need to locate the notebook and upload it to Canvas. The file will be downloaded with the 'ipynb' extension for an iPython Notebook. You'll most likely not be able to open it on your computer, but that's fine as I'll be able to read it in the Jupyter/Colab environment.\n",
    "\n",
    "<b>Stuck?</b> Visit office hours. I'll be able to provide some troubleshooting of a few components but you'll be entirely responsible for responding to the prompts yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d0428-c427-4dd1-9aa9-1a66bc19346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble -- import a number of things that we'll need and install what we won't have\n",
    "!pip install htrc-feature-reader > /dev/null 2>&1\n",
    "!pip install gensim > /dev/null 2>&1\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from htrc_features import FeatureReader, utils  \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.models.keyedvectors as kv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db058c-0642-44d8-9a66-be976dde38eb",
   "metadata": {},
   "source": [
    "## Question #1: Defining Critical AI\n",
    "\n",
    "What are the meaningful differences and similarities between two different (select accounts from different essays/chapters) definitions of \"critical AI\" found in our readings thus far? (Approx. 500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18659bdc-7174-40db-960e-aaf820298e3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82bcae9e-c947-431c-9d9e-e4df9928dfbb",
   "metadata": {},
   "source": [
    "## Question #2: Dataset Critique\n",
    "\n",
    "In Emily Denton, et al., “On the Genealogy of Machine Learning Datasets: A Critical History of ImageNet,” Big Data & Society 8, no. 2 (2021), the authors write: “We analyze discourses which shaped ImageNet, focusing on three problems: the importance of data; meaning and the computational construction of understanding; and the strategic choices regarding the visibility (and invisibility) of labor.\" Please first explain the significance and relation of these three elements and then apply this critical lens to your reading of the [\"Deep Learning Face Attributes in the Wild\"](https://liuziwei7.github.io/projects/FaceAttributes.html) paper and to the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. (Approx 500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d0d1c-5a8d-415f-91dc-147ce9235806",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a99f362-912e-417c-a95b-e35a58a42c2d",
   "metadata": {},
   "source": [
    "## Question #3: Datafiction and Data\n",
    "\n",
    "Using what we have learned about data and datafication, explore the [HathiTrust Digital Library](https://www.hathitrust.org/) and extract the HathiTrust IDs for ten books. You can find the ID by searching for a book on the site (by author name or title, most likely). You will need to click on the link for a specific volume from a specific library. If you want a book that is under copyright protection, you can change \"Item Availbility\" from \"Full View\" to \"All Items\" have you have searched for a book or author. Same process applies for finding the IDs (click on \"Limited (search-only)\" to find ID from the url. Some additional details about the Library can be found on [Wikipedia](https://en.wikipedia.org/wiki/HathiTrust).\n",
    "\n",
    "Now, in approximately 500 words, give an account of how datafiction might be used to understand 1) the HathiTrust Digital Library and 2) the list of texts that you assembled (why does it make sense as a collection? what gives this coherence? etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa7e75-1473-4b50-a619-e306a0a7d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this a list of HathiTrust IDS. Each ID needs to be enclosed in quotation marks and separated with a comma.\n",
    "# Example: 'uc1.32106001535084'\n",
    "documents = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe8c78-4aff-4168-af3b-1477b717d86a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9f4441-06d9-47d7-b253-8248d73b1d8f",
   "metadata": {},
   "source": [
    "## Question #4: Data Model\n",
    "\n",
    "Now that we've created a list of texts, let's train a small neural language model using Doc2Vec on this collection. Run the following cells. In the last cell in this collection, query the model using a several different terms and combination of terms. \n",
    "\n",
    "Then, in approximately 250 words, apply a key concept from Simon Lindgren's Data Theory to the model that you have created. How can you use this concept to understand the model and the responses you've received from the most_similar function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c259b-9de5-4bef-82d5-8d4878c5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts individual pages and create string of words from tokens\n",
    "# Word order is lost from HTRC features. This creates page length strings by\n",
    "# multiplying tokens for each appearance. Thus, token the with count 2 will \n",
    "# appear as \"the the\" in the returned string.\n",
    "\n",
    "def get_pages(document):\n",
    "    fr = FeatureReader([document])\n",
    "    vol = next(fr.volumes())\n",
    "    ptc = (\n",
    "        vol.tokenlist(pos=False, case=False)\n",
    "        .reset_index()\n",
    "        .drop(columns=['section'])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, group in ptc.groupby('page'):\n",
    "        tokens = []\n",
    "        for token, count in zip(group.iloc[:, 1], group.iloc[:, 2]):\n",
    "            if isinstance(token, str) and token.isalpha():\n",
    "                tokens.extend([token] * count)\n",
    "        rows.append(tokens)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec442b-fb8e-4ccf-9e10-c02448eb94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process downloaded features and store as TaggedDocument with a tag for page number\n",
    "# This tage is required for Doc2Vec and would normally be based on paragraphs but we\n",
    "# can only operate on pages of data from HTRC extracted features\n",
    "#\n",
    "\n",
    "pages = list()\n",
    "for d in documents:\n",
    "    for page in get_pages(d):\n",
    "        pages.append(page)\n",
    "\n",
    "tagged_data = [TaggedDocument(words=tokens, tags=[f\"p{i}\"])\n",
    "          for i, tokens in enumerate(pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6116cf5-6316-4329-965b-070dbf1af4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating model\")\n",
    "wvmodel = Doc2Vec(tagged_data,\n",
    "                dm = 1,              # operate on \"paragraphs\" (pages) with distributed memory model\n",
    "                vector_size = 200,   # larger vector size might produce better results but requires more time and memory\n",
    "                min_count = 2,       # drop words with very few repetitions\n",
    "                window = 150,        # larger window size needed because of extracted features\n",
    "                epochs = 10,         # default number of epochs (like did in our Perceptron networks, we'll run all data through multiple times)\n",
    "                workers = 2)         # attempt some parallelism\n",
    "\n",
    "print(\"saving word2vec model\")\n",
    "wvmodel.save_word2vec_format(\"doc2vec-htrc-sample.w2v\")\n",
    "\n",
    "# reload and verify\n",
    "model =  kv.KeyedVectors.load_word2vec_format(\"doc2vec-htrc-sample.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69935ea-1c23-4426-8be7-61671809bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"sea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6785a0-868d-4d7a-8632-8601d0abafb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f513e570-fd2a-44c4-91b8-c5eb3700acb5",
   "metadata": {},
   "source": [
    "## Question #5: Vector Space Similarities in Text Models.\n",
    "\n",
    "First modify the values of the list \"concept_a\" and \"concept_b\" to contain words that you believe to be associate with two distinct concepts. These should be single words and lowercase. Use fairly basic terms--you want them to be in your model's vocabulary. You should have a minimum of five words in each list. What happens if you add words that seem like they could belong to either concept? Then, referencing Mitchell and Gavin, explain the meaning of the resulting scatterplot (Approx. 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973669b4-175e-426a-96ea-8b9ebb8373ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_a = [\"word1\",\"word2\",\"word3\",\"word4\",\"word5\"]\n",
    "concept_b = [\"word1\",\"word2\",\"word3\",\"word4\",\"word5\"]\n",
    "\n",
    "vecs, terms = list(), list()\n",
    "for w in concept_a + concept_b:\n",
    "    if w in model:\n",
    "        vecs.append(model[w])\n",
    "        terms.append(w)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=2, max_iter = 1000, random_state = 42)\n",
    "embeddings_2d = tsne.fit_transform(torch.tensor(vecs))\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "vis = pd.DataFrame({\n",
    "    'TSNE Component 1': embeddings_2d[:, 0],\n",
    "    'TSNE Component 2': embeddings_2d[:, 1],\n",
    "    'Terms': terms,\n",
    "})\n",
    "fig = px.scatter(vis, x = 'TSNE Component 1', \n",
    "                 y = 'TSNE Component 2',\n",
    "                 hover_name = 'Terms',\n",
    "                 hover_data = 'Terms',\n",
    "                 title = \"t-SNE Projection of Embeddings\")\n",
    "fig.update_traces(mode = \"markers\")\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028e8f6-30a8-4b2d-a6ae-95922913e912",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74b600-e407-48bd-854b-320c993b1b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e2a01b-54d3-4b54-a961-1ac840f6caaa",
   "metadata": {},
   "source": [
    "## Question 6: Generative Models\n",
    "\n",
    "We'll now use a instruction-fine tuned version of OLMo-2. Change the contents of the prompt variable and run these cells. Change your prompt based on the outputs that you see. In no more than 500 words, extract a concept from Amoore et al. \"Politics of the Prompt\" and your understanding of Ouyang et al. \"Training Language Models to Follow Instructions with Human Feedback\" to think about the prompting of language models and your specific interactions with OLMo-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714fa84-1d1f-4f1f-adae-c5c963bd2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))\n",
    "\n",
    "model_name = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "olmo_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8bcf6b-9b1c-406e-a981-6de73fc355f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"PROMPT GOES HERE\"\n",
    "\n",
    "max_new_tokens = 128\n",
    "msg = [{\"role\":\"user\",\"content\":prompt}]\n",
    "input_ids = tokenizer.apply_chat_template(msg, \n",
    "                                          return_tensors = \"pt\",\n",
    "                                          add_generation_prompt = False)\n",
    "\n",
    "output = olmo_model.generate(input_ids['input_ids'].to(device), \n",
    "                        do_sample=True, \n",
    "                        max_new_tokens = max_new_tokens,\n",
    "                        temperature = 1.0, \n",
    "                        top_p = 0.95)\n",
    "\n",
    "print(tokenizer.decode(output[0], \n",
    "                       skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b46cd2-8ead-4cf7-911a-fac78f121bf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af673cf8-9af7-4498-a6ea-083b6f0afe61",
   "metadata": {},
   "source": [
    "## Question 7: Consequences of Fair-Use Decision in Bartz v. Anthropic\n",
    "\n",
    "Read back through Judge Alsup's \"Order on Fair Use\" in Bartz v. Anthropic and make an argument, in no more than 500 words, for what you see as the most important consequences of this decision for the future of \"artificial intelligence,\" language modeling, and/or creativity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826650b-b35c-496a-8cae-2701886928b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
